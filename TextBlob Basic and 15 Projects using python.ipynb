{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fea6b3a",
   "metadata": {},
   "source": [
    "# Basic\n",
    "\n",
    "Create a TextBlob         \n",
    "\n",
    "Part-of-speech Tagging\n",
    "\n",
    "Noun Phrase Extraction\n",
    "\n",
    "Sentiment Analysis\n",
    "\n",
    "Tokenization\n",
    "\n",
    "Words Inflection and Lemmatization\n",
    "\n",
    "WordNet Integration\n",
    "\n",
    "WordLists\n",
    "\n",
    "Spelling Correction\n",
    "\n",
    "Get Word and Noun Phrase Frequencies\n",
    "\n",
    "Parsing\n",
    "\n",
    "TextBlobs Are Like Python Strings!\n",
    "\n",
    "n-grams\n",
    "\n",
    "\n",
    "# End to End to Projects\n",
    "\n",
    "Sentiment Analysis on Customer Reviews\n",
    "\n",
    "Language Translation Tool\n",
    "\n",
    "Spell Checker for User Input\n",
    "\n",
    "Keyword Extraction from Text\n",
    "\n",
    "Text Summarization by Sentence Polarity\n",
    "\n",
    "Formality Checker for Text\n",
    "\n",
    "Subjectivity Analysis of Statements\n",
    "\n",
    "Sentence Tokenizer for Paragraphs\n",
    "\n",
    "Text Similarity Checker\n",
    "\n",
    "Text Complexity Scorer\n",
    "\n",
    "Parts of Speech (POS) Tagging\n",
    "\n",
    "Synonym Replacement Tool\n",
    "\n",
    "Custom Word Frequency Counter\n",
    "\n",
    "Named Entity Recognition (NER) using Custom Keywords\n",
    "\n",
    "Politeness Detector for Customer Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28f4b025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\Noor\n",
      "[nltk_data]     Saeed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Noor\n",
      "[nltk_data]     Saeed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Noor\n",
      "[nltk_data]     Saeed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Noor Saeed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package conll2000 to C:\\Users\\Noor\n",
      "[nltk_data]     Saeed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to C:\\Users\\Noor\n",
      "[nltk_data]     Saeed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb1e699",
   "metadata": {},
   "source": [
    "# Create a TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43317443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f4d1f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Python is a high-level, general-purpose programming language.\")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = TextBlob(\"Python is a high-level, general-purpose programming language.\")\n",
    "wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d9b2d",
   "metadata": {},
   "source": [
    "# Parts of speech Taggings\n",
    "\n",
    "Each POS tag represents the grammatical role of the word in the sentence. Here’s the full form of each tag from your list:\n",
    "\n",
    "\n",
    "NNP: Proper Noun, Singular (e.g., \"Python\")\n",
    "\n",
    "VBZ: Verb, 3rd person singular present (e.g., \"is\")\n",
    "\n",
    "DT: Determiner (e.g., \"a\")\n",
    "\n",
    "JJ: Adjective (e.g., \"high-level\", \"general-purpose\")\n",
    "\n",
    "NN: Noun, Singular or Mass (e.g., \"programming,\" \"language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d46f83a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('high-level', 'JJ'),\n",
       " ('general-purpose', 'JJ'),\n",
       " ('programming', 'NN'),\n",
       " ('language', 'NN')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad657d",
   "metadata": {},
   "source": [
    "# Noun Phrase Extraction\n",
    "Similarly, noun phrases are accessed through the noun_phrases property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "304c5129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['python'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b29c3f",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f06490e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "testimonial = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\n",
    "\n",
    "if testimonial.sentiment[0] >=0.1:\n",
    "    print(\"Positive\")\n",
    "else:\n",
    "    print(\"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c48dc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "testimonial = TextBlob(\"I hate to start learning complex concepts first\")\n",
    "\n",
    "if testimonial.sentiment[0] >=0.1:\n",
    "    print(\"Positive\")\n",
    "else:\n",
    "    print(\"Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d43af21",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Word Base\n",
    "\n",
    "Sentence Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "503350cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Beautiful', 'is', 'better', 'than', 'ugly', 'Explicit', 'is', 'better', 'than', 'implicit', 'Simple', 'is', 'better', 'than', 'complex'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen = TextBlob(\n",
    "    \"Beautiful is better than ugly. \"\n",
    "    \"Explicit is better than implicit. \"\n",
    "    \"Simple is better than complex.\"\n",
    ")\n",
    "zen.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b78516d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Beautiful is better than ugly.\"),\n",
       " Sentence(\"Explicit is better than implicit.\"),\n",
       " Sentence(\"Simple is better than complex.\")]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd06a4f",
   "metadata": {},
   "source": [
    "# Words Inflection and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e093b23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Use', '4', 'spaces', 'per', 'indentation', 'level'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = TextBlob(\"Use 4 spaces per indentation level.\")\n",
    "sentence.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6002f7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'space'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.words[2].singularize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6b323de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'levels'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.words[-1].pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ed192ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'place'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word(\"places\")\n",
    "w.lemmatize('n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8b0376e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word(\"went\")\n",
    "w.lemmatize(\"v\")  # Pass in WordNet part of speech (verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab74404b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word(\"loving\")\n",
    "w.lemmatize(\"v\")  # Pass in WordNet part of speech (verb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c195c6",
   "metadata": {},
   "source": [
    "# Wordnet integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31752768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('topographic_point.n.01'),\n",
       " Synset('place.n.02'),\n",
       " Synset('place.n.03'),\n",
       " Synset('place.n.04'),\n",
       " Synset('stead.n.01'),\n",
       " Synset('place.n.06'),\n",
       " Synset('home.n.01'),\n",
       " Synset('position.n.06'),\n",
       " Synset('position.n.01'),\n",
       " Synset('place.n.10'),\n",
       " Synset('seat.n.01'),\n",
       " Synset('place.n.12'),\n",
       " Synset('place.n.13'),\n",
       " Synset('plaza.n.01'),\n",
       " Synset('place.n.15'),\n",
       " Synset('space.n.07'),\n",
       " Synset('put.v.01'),\n",
       " Synset('place.v.02'),\n",
       " Synset('rate.v.01'),\n",
       " Synset('locate.v.03'),\n",
       " Synset('place.v.05'),\n",
       " Synset('place.v.06'),\n",
       " Synset('target.v.01'),\n",
       " Synset('identify.v.01'),\n",
       " Synset('place.v.09'),\n",
       " Synset('set.v.09'),\n",
       " Synset('place.v.11'),\n",
       " Synset('place.v.12'),\n",
       " Synset('invest.v.01'),\n",
       " Synset('station.v.01'),\n",
       " Synset('place.v.15'),\n",
       " Synset('place.v.16')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "from textblob.wordnet import VERB\n",
    "word = Word(\"places\")\n",
    "word.synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00db4d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chop.v.05'),\n",
       " Synset('hack.v.02'),\n",
       " Synset('hack.v.03'),\n",
       " Synset('hack.v.04'),\n",
       " Synset('hack.v.05'),\n",
       " Synset('hack.v.06'),\n",
       " Synset('hack.v.07'),\n",
       " Synset('hack.v.08')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"hack\").get_synsets(pos=VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e110daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a point located with respect to surface features of some region',\n",
       " 'any area set aside for a particular purpose',\n",
       " 'an abstract mental location',\n",
       " 'a general vicinity',\n",
       " 'the post or function properly or customarily occupied or served by another',\n",
       " 'a particular situation',\n",
       " 'where you live at a particular time',\n",
       " 'a job in an organization',\n",
       " 'the particular portion of space occupied by something',\n",
       " 'proper or designated social situation',\n",
       " 'a space reserved for sitting (as in a theater or on a train or airplane)',\n",
       " 'the passage that is being read',\n",
       " 'proper or appropriate position or location',\n",
       " 'a public square with room for pedestrians',\n",
       " 'an item on a list or in a sequence',\n",
       " 'a blank area',\n",
       " 'put into a certain place or abstract location',\n",
       " 'place somebody in a particular situation or location',\n",
       " 'assign a rank or rating to',\n",
       " 'assign a location to',\n",
       " 'to arrange for',\n",
       " 'take a place in a competition; often followed by an ordinal',\n",
       " 'intend (something) to move towards a certain goal',\n",
       " 'recognize as being; establish the identity of someone or something',\n",
       " 'assign to (a job or a home)',\n",
       " 'locate',\n",
       " 'estimate',\n",
       " 'identify the location or place of',\n",
       " 'make an investment',\n",
       " 'assign to a station',\n",
       " 'finish second or better in a horse or dog race',\n",
       " 'sing a note with the correct pitch']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"Place\").definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69099aa",
   "metadata": {},
   "source": [
    "# Spelling Correction & Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c388a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have good spelling!\n"
     ]
    }
   ],
   "source": [
    "b = TextBlob(\"I havv goood speling!\")\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8284d94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lovely', 0.44776119402985076),\n",
       " ('lover', 0.3880597014925373),\n",
       " ('lovers', 0.13432835820895522),\n",
       " ('livery', 0.029850746268656716)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word(\"lovery\")\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee84a676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nice', 0.9636363636363636),\n",
       " ('nina', 0.01818181818181818),\n",
       " ('nick', 0.01818181818181818)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word(\"nica\")\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a3f1b",
   "metadata": {},
   "source": [
    "# Parsing\n",
    "\n",
    "Here's the breakdown of the tags in your example:\n",
    "\n",
    "Word: The actual word from the sentence.\n",
    "\n",
    "POS-tag: Part-of-speech tag, indicating the grammatical role of the word (e.g., \"CC\" for coordinating conjunction, \"RB\" for adverb, etc.).\n",
    "\n",
    "Chunk-tag: Indicates the beginning or continuation of a chunk (e.g., noun phrases, prepositional phrases). This uses B (beginning) and I (inside) tags to show chunk boundaries:\n",
    "\n",
    "B-NP: Beginning of a noun phrase\n",
    "\n",
    "I-NP: Inside a noun phrase\n",
    "\n",
    "B-PP: Beginning of a prepositional phrase\n",
    "\n",
    "B-ADVP: Beginning of an adverbial phrase\n",
    "\n",
    "O: Outside of any chunk\n",
    "\n",
    "Named-entity-tag: Marks named entities (e.g., organizations, locations) with specific tags. In your example, everything is tagged as O (Outside any named entity), meaning there are no named entities in this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a08ec4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And/CC/O/O now/RB/B-ADVP/O for/IN/B-PP/B-PNP something/NN/B-NP/I-PNP completely/RB/B-ADJP/O different/JJ/I-ADJP/O ././O/O\n"
     ]
    }
   ],
   "source": [
    "b = TextBlob(\"And now for something completely different.\")\n",
    "print(b.parse())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac508c",
   "metadata": {},
   "source": [
    "# n-gram (gram, bi-gram, tri-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09b9faba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Now', 'is', 'better']),\n",
       " WordList(['is', 'better', 'than']),\n",
       " WordList(['better', 'than', 'never'])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"Now is better than never.\")\n",
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1c6fba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Now']),\n",
       " WordList(['is']),\n",
       " WordList(['better']),\n",
       " WordList(['than']),\n",
       " WordList(['never'])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69195a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Now', 'is']),\n",
       " WordList(['is', 'better']),\n",
       " WordList(['better', 'than']),\n",
       " WordList(['than', 'never'])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360f56a",
   "metadata": {},
   "source": [
    "# Project 1: Sentiment Analysis on Customer Reviews\n",
    "\n",
    "\n",
    "two possibilies\n",
    "\n",
    "Real Time Sentiment Detection\n",
    "\n",
    "Detection Sentiment from text (custom data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed799fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your text......or press exit to leave   I love to watch movies\n",
      "\n",
      "\n",
      "Positive\n",
      "Enter your text......or press exit to leave   we don't like this movies its so ugly\n",
      "\n",
      "\n",
      "negative\n",
      "Enter your text......or press exit to leave   i wanna go to USA\n",
      "\n",
      "\n",
      "Neutral\n",
      "Enter your text......or press exit to leave   we loves each other\n",
      "\n",
      "\n",
      "negative\n",
      "Enter your text......or press exit to leave   we love each other\n",
      "\n",
      "\n",
      "Positive\n",
      "Enter your text......or press exit to leave   exit\n",
      "\n",
      "\n",
      "Neutral\n",
      "Good Bye\n"
     ]
    }
   ],
   "source": [
    "# Real Time Detection\n",
    "from textblob import TextBlob\n",
    "while True:\n",
    "    input_text = input(\"Enter your text......or press exit to leave   \")\n",
    "    print(\"\\n\")\n",
    "    text = TextBlob(input_text)\n",
    "    sentiment = text.sentiment.polarity\n",
    "    if sentiment > 0:\n",
    "        print(\"Positive\")\n",
    "    elif sentiment==0:\n",
    "        print(\"Neutral\")\n",
    "    else:\n",
    "        print(\"negative\")\n",
    "    \n",
    "    if input_text == \"exit\":\n",
    "        print(\"Good Bye\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1ff5283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on custom data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5995c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_detection(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = TextBlob(text)\n",
    "    \n",
    "    sentiment = text.sentiment.polarity\n",
    "    \n",
    "    if sentiment > 0:\n",
    "        return \"positive\"\n",
    "    elif sentiment < 0:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87651e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer name</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rishikumar Thakur</td>\n",
       "      <td>Another Midrange killer Smartphone by Xiaomi\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Raza ji</td>\n",
       "      <td>All ok but vry small size mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vaibhav Patel</td>\n",
       "      <td>Quite good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>Redmi has always have been the the king of bud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sudhakaran Wadakkancheri</td>\n",
       "      <td>worst product from MI. I am a hardcore fan of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Customer name                                           Comments\n",
       "0         Rishikumar Thakur  Another Midrange killer Smartphone by Xiaomi\\n...\n",
       "1                   Raza ji                   All ok but vry small size mobile\n",
       "2             Vaibhav Patel                                         Quite good\n",
       "3           Amazon Customer  Redmi has always have been the the king of bud...\n",
       "4  Sudhakaran Wadakkancheri  worst product from MI. I am a hardcore fan of ..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"redmi6.csv\", encoding='ISO-8859-1')\n",
    "df = df[['Customer name','Comments']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40f3f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['Comments'].apply(sentiment_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dacbb9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer name</th>\n",
       "      <th>Comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rishikumar Thakur</td>\n",
       "      <td>Another Midrange killer Smartphone by Xiaomi\\n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Raza ji</td>\n",
       "      <td>All ok but vry small size mobile</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vaibhav Patel</td>\n",
       "      <td>Quite good</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>Redmi has always have been the the king of bud...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sudhakaran Wadakkancheri</td>\n",
       "      <td>worst product from MI. I am a hardcore fan of ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Rahul</td>\n",
       "      <td>I like This Phone, Awesome look and design.\\nI...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>Sunil Soni</td>\n",
       "      <td>Product is avasome but invoice is note include...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>D.C.Padhi</td>\n",
       "      <td>Redmi Note4, Note5, now 6pro..It seems the old...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>Mahesh</td>\n",
       "      <td>I love mi</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>Vinod</td>\n",
       "      <td>Same old configurations with higher price.\\nNo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Customer name  \\\n",
       "0           Rishikumar Thakur   \n",
       "1                     Raza ji   \n",
       "2               Vaibhav Patel   \n",
       "3             Amazon Customer   \n",
       "4    Sudhakaran Wadakkancheri   \n",
       "..                        ...   \n",
       "275                     Rahul   \n",
       "276                Sunil Soni   \n",
       "277                 D.C.Padhi   \n",
       "278                    Mahesh   \n",
       "279                     Vinod   \n",
       "\n",
       "                                              Comments     label  \n",
       "0    Another Midrange killer Smartphone by Xiaomi\\n...  positive  \n",
       "1                     All ok but vry small size mobile  positive  \n",
       "2                                           Quite good  positive  \n",
       "3    Redmi has always have been the the king of bud...  positive  \n",
       "4    worst product from MI. I am a hardcore fan of ...  negative  \n",
       "..                                                 ...       ...  \n",
       "275  I like This Phone, Awesome look and design.\\nI...  positive  \n",
       "276  Product is avasome but invoice is note include...   neutral  \n",
       "277  Redmi Note4, Note5, now 6pro..It seems the old...  positive  \n",
       "278                                          I love mi  positive  \n",
       "279  Same old configurations with higher price.\\nNo...  positive  \n",
       "\n",
       "[280 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e659f",
   "metadata": {},
   "source": [
    "# Project 2: Language Translation Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6219746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# language translation is deprecated from textblob but we can use alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3a5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob import TextBlob\n",
    "\n",
    "# blob = TextBlob('TextBlob is a great tool for developers')\n",
    "# print(blob.translate(to='hi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1698ce",
   "metadata": {},
   "source": [
    "# Deep Translator (Google Translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21303cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je m'appelle noor saeed\n"
     ]
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "text = \"my name is noor saeed\"\n",
    "print(GoogleTranslator(source='en', target='fr').translate(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "269d444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(text, target_lang):\n",
    "    trans_text = GoogleTranslator(target=target_lang).translate(text)\n",
    "    return trans_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2dd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Language Translation tool...\n",
      "Enter your text....we appreciate each other\n",
      "To translate lang....ur\n",
      "Translation : ہم ایک دوسرے کی تعریف کرتے ہیں\n",
      "\n",
      " Language Translation tool...\n",
      "Enter your text....this is a cat\n",
      "To translate lang....hi\n",
      "Translation : यह एक बिल्ली है\n",
      "\n",
      " Language Translation tool...\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(\"\\n Language Translation tool...\")\n",
    "    input_text = input(\"Enter your text....\")\n",
    "    \n",
    "    if input_text == \"exit\":\n",
    "        break\n",
    "        \n",
    "    target_lang = input(\"To translate lang....\")\n",
    "    \n",
    "    trans_text = trans(input_text,target_lang)\n",
    "    print(\"Translation :\", trans_text)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b461f8",
   "metadata": {},
   "source": [
    "# Project 3: Spell Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81bcdb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love you\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "mytxt = \"i lov you\"\n",
    "text = TextBlob(mytxt)\n",
    "\n",
    "print(text.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f49f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries (Run this code in VS code or pycharm)\n",
    "import streamlit as st\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Streamlit app setup\n",
    "st.title(\"Real-Time Spell Checker\")\n",
    "st.write(\"Enter text with spelling errors below, and see the corrected version in real time.\")\n",
    "\n",
    "# Text input\n",
    "text = st.text_area(\"Type your text here:\", \"\")\n",
    "\n",
    "# Check if the input text is not empty\n",
    "if text:\n",
    "    # Use TextBlob for spell correction\n",
    "    blob = TextBlob(text)\n",
    "    corrected_text = blob.correct()\n",
    "    \n",
    "    # Display the results\n",
    "    st.subheader(\"Corrected Text\")\n",
    "    st.write(corrected_text)\n",
    "\n",
    "# Info\n",
    "st.info(\"This app uses TextBlob for spell checking and correction.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095eccf7",
   "metadata": {},
   "source": [
    "# Project 4: Auto Keyword Extraction from Articles Text Using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d91c3043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>We introduce bag context, a device for regulat...</td>\n",
       "      <td>['Frank Drewes', 'Christine du Toit', 'Sigrid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>In addition to science citation indicators of ...</td>\n",
       "      <td>['Loet Leydesdorff']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>Player strategy pattern recognition (PSPR) is ...</td>\n",
       "      <td>['Suoju He', 'Yi Wang', 'F. X. Xie', 'Jin Meng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>Cinematic virtual reality (VR) aims to provide...</td>\n",
       "      <td>['Jayant Thatte', 'Jean-Baptiste Boin', 'Haric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>With the increasing demand and popularity of m...</td>\n",
       "      <td>['Bin Yu', 'M. Nahrstedt']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Motivation: Most proteins consist of multiple ...</td>\n",
       "      <td>['Dong Xu', 'Lukasz Jaroszewski', 'Zhanwen Li'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>We give here a new, topological, definition of...</td>\n",
       "      <td>['Emmanuel Jeandel']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>In the process of mapping compute-intensive al...</td>\n",
       "      <td>['Sebastian Siegel', 'Renate Merker']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>Android applications often rely on alarms to s...</td>\n",
       "      <td>['Mario Almeida', 'Muhammad Bilal', 'Jeremy Bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>We consider deterministic anonymous distribute...</td>\n",
       "      <td>['Julien M. Hendrickx', 'John N. Tsitsiklis']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Distributed Denial of Service attacks have rec...</td>\n",
       "      <td>['Arjan Durresi', 'Vamsi Paruchuri']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>Increasing energy consumption of commercial bu...</td>\n",
       "      <td>['Shailja Thakur', 'Manaswi Saha', 'Amarjeet S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Integration of many functions in mobile device...</td>\n",
       "      <td>['Masayuki Moriguchi', 'Arata Shinozaki', 'Tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>Just as special purpose computers and mainfram...</td>\n",
       "      <td>[\"Keith O'Hara\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>In this paper, we propose a semi-closed microc...</td>\n",
       "      <td>['Masaru Takeuchi', 'Masahiro Nakajima', 'Tosh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>A new way to improve the classification rate o...</td>\n",
       "      <td>['Quentin Noirhomme', 'Richard Kitney', 'Benoî...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>This paper describes a study of the effects of...</td>\n",
       "      <td>['Basabdatta Sen Bhattacharya', 'Damien Coyle'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>The development of a monitor based on guided-t...</td>\n",
       "      <td>['Scott Gneiting', 'Daniel E. Smalley', 'Kamra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>In this paper, we propose a uniform quantizati...</td>\n",
       "      <td>['Qifeng Gan', 'J.M.P. Langlois', 'Yvon Savaria']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>High power dissipation during scan-based logic...</td>\n",
       "      <td>['Takaaki Kato', 'Senling Wang', 'Yasuo Sato',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                           abstract  \\\n",
       "0            2  We introduce bag context, a device for regulat...   \n",
       "1           11  In addition to science citation indicators of ...   \n",
       "2            7  Player strategy pattern recognition (PSPR) is ...   \n",
       "3           14  Cinematic virtual reality (VR) aims to provide...   \n",
       "4           15  With the increasing demand and popularity of m...   \n",
       "5            5  Motivation: Most proteins consist of multiple ...   \n",
       "6           12  We give here a new, topological, definition of...   \n",
       "7            6  In the process of mapping compute-intensive al...   \n",
       "8           19  Android applications often rely on alarms to s...   \n",
       "9           17  We consider deterministic anonymous distribute...   \n",
       "10          10  Distributed Denial of Service attacks have rec...   \n",
       "11          13  Increasing energy consumption of commercial bu...   \n",
       "12           1  Integration of many functions in mobile device...   \n",
       "13           4  Just as special purpose computers and mainfram...   \n",
       "14          16  In this paper, we propose a semi-closed microc...   \n",
       "15           9  A new way to improve the classification rate o...   \n",
       "16           8  This paper describes a study of the effects of...   \n",
       "17           0  The development of a monitor based on guided-t...   \n",
       "18          18  In this paper, we propose a uniform quantizati...   \n",
       "19           3  High power dissipation during scan-based logic...   \n",
       "\n",
       "                                              authors  \n",
       "0   ['Frank Drewes', 'Christine du Toit', 'Sigrid ...  \n",
       "1                                ['Loet Leydesdorff']  \n",
       "2   ['Suoju He', 'Yi Wang', 'F. X. Xie', 'Jin Meng...  \n",
       "3   ['Jayant Thatte', 'Jean-Baptiste Boin', 'Haric...  \n",
       "4                          ['Bin Yu', 'M. Nahrstedt']  \n",
       "5   ['Dong Xu', 'Lukasz Jaroszewski', 'Zhanwen Li'...  \n",
       "6                                ['Emmanuel Jeandel']  \n",
       "7               ['Sebastian Siegel', 'Renate Merker']  \n",
       "8   ['Mario Almeida', 'Muhammad Bilal', 'Jeremy Bl...  \n",
       "9       ['Julien M. Hendrickx', 'John N. Tsitsiklis']  \n",
       "10               ['Arjan Durresi', 'Vamsi Paruchuri']  \n",
       "11  ['Shailja Thakur', 'Manaswi Saha', 'Amarjeet S...  \n",
       "12  ['Masayuki Moriguchi', 'Arata Shinozaki', 'Tak...  \n",
       "13                                   [\"Keith O'Hara\"]  \n",
       "14  ['Masaru Takeuchi', 'Masahiro Nakajima', 'Tosh...  \n",
       "15  ['Quentin Noirhomme', 'Richard Kitney', 'Benoî...  \n",
       "16  ['Basabdatta Sen Bhattacharya', 'Damien Coyle'...  \n",
       "17  ['Scott Gneiting', 'Daniel E. Smalley', 'Kamra...  \n",
       "18  ['Qifeng Gan', 'J.M.P. Langlois', 'Yvon Savaria']  \n",
       "19  ['Takaaki Kato', 'Senling Wang', 'Yasuo Sato',...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"small_paper_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf2986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b9ec68e",
   "metadata": {},
   "source": [
    "# clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a40c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "275e4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # lower casing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # removing punctuations, special chars etc\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    \n",
    "    # tokinzation\n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    # stopwords removing\n",
    "    text = [w for w in text if w not in stopwords]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a0c1286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine learning enables 890808 computers learn data without explicitly programmed'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Machine! learning enables 890808 @#$#@$#@ computers to? learn from data without being explicitly programmed.\"\n",
    "\n",
    "clean(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5aae24d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>cleaned_abtract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>We introduce bag context, a device for regulat...</td>\n",
       "      <td>['Frank Drewes', 'Christine du Toit', 'Sigrid ...</td>\n",
       "      <td>introduce bag context device regulated rewriti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>In addition to science citation indicators of ...</td>\n",
       "      <td>['Loet Leydesdorff']</td>\n",
       "      <td>addition science citation indicators journals ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>Player strategy pattern recognition (PSPR) is ...</td>\n",
       "      <td>['Suoju He', 'Yi Wang', 'F. X. Xie', 'Jin Meng...</td>\n",
       "      <td>player strategy pattern recognition pspr apply...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>Cinematic virtual reality (VR) aims to provide...</td>\n",
       "      <td>['Jayant Thatte', 'Jean-Baptiste Boin', 'Haric...</td>\n",
       "      <td>cinematic virtual reality vr aims provide imme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>With the increasing demand and popularity of m...</td>\n",
       "      <td>['Bin Yu', 'M. Nahrstedt']</td>\n",
       "      <td>increasing demand popularity multimedia stream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Motivation: Most proteins consist of multiple ...</td>\n",
       "      <td>['Dong Xu', 'Lukasz Jaroszewski', 'Zhanwen Li'...</td>\n",
       "      <td>motivation proteins consist multiple domains i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>We give here a new, topological, definition of...</td>\n",
       "      <td>['Emmanuel Jeandel']</td>\n",
       "      <td>give new topological definition automata exten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>In the process of mapping compute-intensive al...</td>\n",
       "      <td>['Sebastian Siegel', 'Renate Merker']</td>\n",
       "      <td>process mapping computeintensive algorithms on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>Android applications often rely on alarms to s...</td>\n",
       "      <td>['Mario Almeida', 'Muhammad Bilal', 'Jeremy Bl...</td>\n",
       "      <td>android applications often rely alarms schedul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>We consider deterministic anonymous distribute...</td>\n",
       "      <td>['Julien M. Hendrickx', 'John N. Tsitsiklis']</td>\n",
       "      <td>consider deterministic anonymous distributed s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Distributed Denial of Service attacks have rec...</td>\n",
       "      <td>['Arjan Durresi', 'Vamsi Paruchuri']</td>\n",
       "      <td>distributed denial service attacks recently em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>Increasing energy consumption of commercial bu...</td>\n",
       "      <td>['Shailja Thakur', 'Manaswi Saha', 'Amarjeet S...</td>\n",
       "      <td>increasing energy consumption commercial build...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Integration of many functions in mobile device...</td>\n",
       "      <td>['Masayuki Moriguchi', 'Arata Shinozaki', 'Tak...</td>\n",
       "      <td>integration many functions mobile devices ofte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>Just as special purpose computers and mainfram...</td>\n",
       "      <td>[\"Keith O'Hara\"]</td>\n",
       "      <td>special purpose computers mainframes grew gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>In this paper, we propose a semi-closed microc...</td>\n",
       "      <td>['Masaru Takeuchi', 'Masahiro Nakajima', 'Tosh...</td>\n",
       "      <td>paper propose semiclosed microchip realize pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>A new way to improve the classification rate o...</td>\n",
       "      <td>['Quentin Noirhomme', 'Richard Kitney', 'Benoî...</td>\n",
       "      <td>new way improve classification rate eegbased b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>This paper describes a study of the effects of...</td>\n",
       "      <td>['Basabdatta Sen Bhattacharya', 'Damien Coyle'...</td>\n",
       "      <td>paper describes study effects variation synapt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>The development of a monitor based on guided-t...</td>\n",
       "      <td>['Scott Gneiting', 'Daniel E. Smalley', 'Kamra...</td>\n",
       "      <td>development monitor based guidedtoleakymode wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>In this paper, we propose a uniform quantizati...</td>\n",
       "      <td>['Qifeng Gan', 'J.M.P. Langlois', 'Yvon Savaria']</td>\n",
       "      <td>paper propose uniform quantization likelihood ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>High power dissipation during scan-based logic...</td>\n",
       "      <td>['Takaaki Kato', 'Senling Wang', 'Yasuo Sato',...</td>\n",
       "      <td>high power dissipation scanbased logic bist cr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                           abstract  \\\n",
       "0            2  We introduce bag context, a device for regulat...   \n",
       "1           11  In addition to science citation indicators of ...   \n",
       "2            7  Player strategy pattern recognition (PSPR) is ...   \n",
       "3           14  Cinematic virtual reality (VR) aims to provide...   \n",
       "4           15  With the increasing demand and popularity of m...   \n",
       "5            5  Motivation: Most proteins consist of multiple ...   \n",
       "6           12  We give here a new, topological, definition of...   \n",
       "7            6  In the process of mapping compute-intensive al...   \n",
       "8           19  Android applications often rely on alarms to s...   \n",
       "9           17  We consider deterministic anonymous distribute...   \n",
       "10          10  Distributed Denial of Service attacks have rec...   \n",
       "11          13  Increasing energy consumption of commercial bu...   \n",
       "12           1  Integration of many functions in mobile device...   \n",
       "13           4  Just as special purpose computers and mainfram...   \n",
       "14          16  In this paper, we propose a semi-closed microc...   \n",
       "15           9  A new way to improve the classification rate o...   \n",
       "16           8  This paper describes a study of the effects of...   \n",
       "17           0  The development of a monitor based on guided-t...   \n",
       "18          18  In this paper, we propose a uniform quantizati...   \n",
       "19           3  High power dissipation during scan-based logic...   \n",
       "\n",
       "                                              authors  \\\n",
       "0   ['Frank Drewes', 'Christine du Toit', 'Sigrid ...   \n",
       "1                                ['Loet Leydesdorff']   \n",
       "2   ['Suoju He', 'Yi Wang', 'F. X. Xie', 'Jin Meng...   \n",
       "3   ['Jayant Thatte', 'Jean-Baptiste Boin', 'Haric...   \n",
       "4                          ['Bin Yu', 'M. Nahrstedt']   \n",
       "5   ['Dong Xu', 'Lukasz Jaroszewski', 'Zhanwen Li'...   \n",
       "6                                ['Emmanuel Jeandel']   \n",
       "7               ['Sebastian Siegel', 'Renate Merker']   \n",
       "8   ['Mario Almeida', 'Muhammad Bilal', 'Jeremy Bl...   \n",
       "9       ['Julien M. Hendrickx', 'John N. Tsitsiklis']   \n",
       "10               ['Arjan Durresi', 'Vamsi Paruchuri']   \n",
       "11  ['Shailja Thakur', 'Manaswi Saha', 'Amarjeet S...   \n",
       "12  ['Masayuki Moriguchi', 'Arata Shinozaki', 'Tak...   \n",
       "13                                   [\"Keith O'Hara\"]   \n",
       "14  ['Masaru Takeuchi', 'Masahiro Nakajima', 'Tosh...   \n",
       "15  ['Quentin Noirhomme', 'Richard Kitney', 'Benoî...   \n",
       "16  ['Basabdatta Sen Bhattacharya', 'Damien Coyle'...   \n",
       "17  ['Scott Gneiting', 'Daniel E. Smalley', 'Kamra...   \n",
       "18  ['Qifeng Gan', 'J.M.P. Langlois', 'Yvon Savaria']   \n",
       "19  ['Takaaki Kato', 'Senling Wang', 'Yasuo Sato',...   \n",
       "\n",
       "                                      cleaned_abtract  \n",
       "0   introduce bag context device regulated rewriti...  \n",
       "1   addition science citation indicators journals ...  \n",
       "2   player strategy pattern recognition pspr apply...  \n",
       "3   cinematic virtual reality vr aims provide imme...  \n",
       "4   increasing demand popularity multimedia stream...  \n",
       "5   motivation proteins consist multiple domains i...  \n",
       "6   give new topological definition automata exten...  \n",
       "7   process mapping computeintensive algorithms on...  \n",
       "8   android applications often rely alarms schedul...  \n",
       "9   consider deterministic anonymous distributed s...  \n",
       "10  distributed denial service attacks recently em...  \n",
       "11  increasing energy consumption commercial build...  \n",
       "12  integration many functions mobile devices ofte...  \n",
       "13  special purpose computers mainframes grew gene...  \n",
       "14  paper propose semiclosed microchip realize pro...  \n",
       "15  new way improve classification rate eegbased b...  \n",
       "16  paper describes study effects variation synapt...  \n",
       "17  development monitor based guidedtoleakymode wa...  \n",
       "18  paper propose uniform quantization likelihood ...  \n",
       "19  high power dissipation scanbased logic bist cr...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_abtract'] = df['abstract'].apply(clean)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4401fcdf",
   "metadata": {},
   "source": [
    "Explanation (bellow code)\n",
    "\n",
    "Nouns: Captures both common and proper nouns (singular and plural).\n",
    "    \n",
    "Adjectives: Includes comparative (JJR) and superlative (JJS) forms in addition to base adjectives (JJ).\n",
    "    \n",
    "Verbs: Includes various forms (base, past, gerund, participle, etc.).\n",
    "    \n",
    "Adverbs: Includes comparative (RBR) and superlative (RBS) adverbs as well as standard adverbs (RB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5355b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_keywords(text):\n",
    "    # Create a TextBlob object\n",
    "    blob = TextBlob(text)\n",
    "\n",
    "    # Extract a broader range of keywords based on POS tagging\n",
    "    keywords = [word for word, tag in blob.tags if tag in ('NN', 'NNS', 'NNP', 'NNPS',  # Nouns\n",
    "                                                           'JJ', 'JJR', 'JJS',          # Adjectives\n",
    "                                                           'RB', 'RBR', 'RBS')]         # Adverbs\n",
    "\n",
    "    # Count the most common keywords\n",
    "    keyword_counts = Counter(keywords)\n",
    "    most_common_keywords = keyword_counts.most_common(5)  # Get the top 5 most common keywords\n",
    "    \n",
    "    return most_common_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6550744",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keywords'] = df['cleaned_abtract'].apply(get_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a369678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>cleaned_abtract</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>We introduce bag context, a device for regulat...</td>\n",
       "      <td>['Frank Drewes', 'Christine du Toit', 'Sigrid ...</td>\n",
       "      <td>introduce bag context device regulated rewriti...</td>\n",
       "      <td>[(tree, 8), (class, 4), (context, 3), (bc, 3),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>In addition to science citation indicators of ...</td>\n",
       "      <td>['Loet Leydesdorff']</td>\n",
       "      <td>addition science citation indicators journals ...</td>\n",
       "      <td>[(citation, 8), (journals, 4), (centrality, 4)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>Player strategy pattern recognition (PSPR) is ...</td>\n",
       "      <td>['Suoju He', 'Yi Wang', 'F. X. Xie', 'Jin Meng...</td>\n",
       "      <td>player strategy pattern recognition pspr apply...</td>\n",
       "      <td>[(game, 6), (ai, 4), (strategy, 3), (pspr, 3),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>Cinematic virtual reality (VR) aims to provide...</td>\n",
       "      <td>['Jayant Thatte', 'Jean-Baptiste Boin', 'Haric...</td>\n",
       "      <td>cinematic virtual reality vr aims provide imme...</td>\n",
       "      <td>[(stereo, 3), (cues, 3), (dasp, 3), (cinematic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>With the increasing demand and popularity of m...</td>\n",
       "      <td>['Bin Yu', 'M. Nahrstedt']</td>\n",
       "      <td>increasing demand popularity multimedia stream...</td>\n",
       "      <td>[(stream, 3), (software, 2), (problem, 2), (ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Motivation: Most proteins consist of multiple ...</td>\n",
       "      <td>['Dong Xu', 'Lukasz Jaroszewski', 'Zhanwen Li'...</td>\n",
       "      <td>motivation proteins consist multiple domains i...</td>\n",
       "      <td>[(domains, 5), (protein, 4), (domain, 4), (oft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>We give here a new, topological, definition of...</td>\n",
       "      <td>['Emmanuel Jeandel']</td>\n",
       "      <td>give new topological definition automata exten...</td>\n",
       "      <td>[(automata, 3), (probabilistic, 2), (quantum, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>In the process of mapping compute-intensive al...</td>\n",
       "      <td>['Sebastian Siegel', 'Renate Merker']</td>\n",
       "      <td>process mapping computeintensive algorithms on...</td>\n",
       "      <td>[(channels, 4), (registers, 4), (pes, 3), (alg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>Android applications often rely on alarms to s...</td>\n",
       "      <td>['Mario Almeida', 'Muhammad Bilal', 'Jeremy Bl...</td>\n",
       "      <td>android applications often rely alarms schedul...</td>\n",
       "      <td>[(applications, 5), (alarms, 4), (android, 3),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>We consider deterministic anonymous distribute...</td>\n",
       "      <td>['Julien M. Hendrickx', 'John N. Tsitsiklis']</td>\n",
       "      <td>consider deterministic anonymous distributed s...</td>\n",
       "      <td>[(nodes, 3), (communications, 2), (functions, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Distributed Denial of Service attacks have rec...</td>\n",
       "      <td>['Arjan Durresi', 'Vamsi Paruchuri']</td>\n",
       "      <td>distributed denial service attacks recently em...</td>\n",
       "      <td>[(attacks, 3), (attack, 3), (hierarchical, 3),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>Increasing energy consumption of commercial bu...</td>\n",
       "      <td>['Shailja Thakur', 'Manaswi Saha', 'Amarjeet S...</td>\n",
       "      <td>increasing energy consumption commercial build...</td>\n",
       "      <td>[(energy, 11), (apportionment, 4), (wattshare,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Integration of many functions in mobile device...</td>\n",
       "      <td>['Masayuki Moriguchi', 'Arata Shinozaki', 'Tak...</td>\n",
       "      <td>integration many functions mobile devices ofte...</td>\n",
       "      <td>[(system, 7), (granularity, 5), (data, 4), (mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>Just as special purpose computers and mainfram...</td>\n",
       "      <td>[\"Keith O'Hara\"]</td>\n",
       "      <td>special purpose computers mainframes grew gene...</td>\n",
       "      <td>[(purpose, 3), (robot, 3), (special, 2), (comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>In this paper, we propose a semi-closed microc...</td>\n",
       "      <td>['Masaru Takeuchi', 'Masahiro Nakajima', 'Tosh...</td>\n",
       "      <td>paper propose semiclosed microchip realize pro...</td>\n",
       "      <td>[(cell, 9), (microchip, 8), (bath, 7), (cultur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>A new way to improve the classification rate o...</td>\n",
       "      <td>['Quentin Noirhomme', 'Richard Kitney', 'Benoî...</td>\n",
       "      <td>new way improve classification rate eegbased b...</td>\n",
       "      <td>[(bci, 5), (results, 4), (methods, 3), (classi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>This paper describes a study of the effects of...</td>\n",
       "      <td>['Basabdatta Sen Bhattacharya', 'Damien Coyle'...</td>\n",
       "      <td>paper describes study effects variation synapt...</td>\n",
       "      <td>[(model, 6), (synaptic, 4), (certain, 4), (eeg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>The development of a monitor based on guided-t...</td>\n",
       "      <td>['Scott Gneiting', 'Daniel E. Smalley', 'Kamra...</td>\n",
       "      <td>development monitor based guidedtoleakymode wa...</td>\n",
       "      <td>[(input, 2), (materials, 2), (wave, 2), (devel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>In this paper, we propose a uniform quantizati...</td>\n",
       "      <td>['Qifeng Gan', 'J.M.P. Langlois', 'Yvon Savaria']</td>\n",
       "      <td>paper propose uniform quantization likelihood ...</td>\n",
       "      <td>[(implementation, 5), (uqle, 4), (algorithm, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>High power dissipation during scan-based logic...</td>\n",
       "      <td>['Takaaki Kato', 'Senling Wang', 'Yasuo Sato',...</td>\n",
       "      <td>high power dissipation scanbased logic bist cr...</td>\n",
       "      <td>[(test, 5), (power, 4), (control, 4), (pattern...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                           abstract  \\\n",
       "0            2  We introduce bag context, a device for regulat...   \n",
       "1           11  In addition to science citation indicators of ...   \n",
       "2            7  Player strategy pattern recognition (PSPR) is ...   \n",
       "3           14  Cinematic virtual reality (VR) aims to provide...   \n",
       "4           15  With the increasing demand and popularity of m...   \n",
       "5            5  Motivation: Most proteins consist of multiple ...   \n",
       "6           12  We give here a new, topological, definition of...   \n",
       "7            6  In the process of mapping compute-intensive al...   \n",
       "8           19  Android applications often rely on alarms to s...   \n",
       "9           17  We consider deterministic anonymous distribute...   \n",
       "10          10  Distributed Denial of Service attacks have rec...   \n",
       "11          13  Increasing energy consumption of commercial bu...   \n",
       "12           1  Integration of many functions in mobile device...   \n",
       "13           4  Just as special purpose computers and mainfram...   \n",
       "14          16  In this paper, we propose a semi-closed microc...   \n",
       "15           9  A new way to improve the classification rate o...   \n",
       "16           8  This paper describes a study of the effects of...   \n",
       "17           0  The development of a monitor based on guided-t...   \n",
       "18          18  In this paper, we propose a uniform quantizati...   \n",
       "19           3  High power dissipation during scan-based logic...   \n",
       "\n",
       "                                              authors  \\\n",
       "0   ['Frank Drewes', 'Christine du Toit', 'Sigrid ...   \n",
       "1                                ['Loet Leydesdorff']   \n",
       "2   ['Suoju He', 'Yi Wang', 'F. X. Xie', 'Jin Meng...   \n",
       "3   ['Jayant Thatte', 'Jean-Baptiste Boin', 'Haric...   \n",
       "4                          ['Bin Yu', 'M. Nahrstedt']   \n",
       "5   ['Dong Xu', 'Lukasz Jaroszewski', 'Zhanwen Li'...   \n",
       "6                                ['Emmanuel Jeandel']   \n",
       "7               ['Sebastian Siegel', 'Renate Merker']   \n",
       "8   ['Mario Almeida', 'Muhammad Bilal', 'Jeremy Bl...   \n",
       "9       ['Julien M. Hendrickx', 'John N. Tsitsiklis']   \n",
       "10               ['Arjan Durresi', 'Vamsi Paruchuri']   \n",
       "11  ['Shailja Thakur', 'Manaswi Saha', 'Amarjeet S...   \n",
       "12  ['Masayuki Moriguchi', 'Arata Shinozaki', 'Tak...   \n",
       "13                                   [\"Keith O'Hara\"]   \n",
       "14  ['Masaru Takeuchi', 'Masahiro Nakajima', 'Tosh...   \n",
       "15  ['Quentin Noirhomme', 'Richard Kitney', 'Benoî...   \n",
       "16  ['Basabdatta Sen Bhattacharya', 'Damien Coyle'...   \n",
       "17  ['Scott Gneiting', 'Daniel E. Smalley', 'Kamra...   \n",
       "18  ['Qifeng Gan', 'J.M.P. Langlois', 'Yvon Savaria']   \n",
       "19  ['Takaaki Kato', 'Senling Wang', 'Yasuo Sato',...   \n",
       "\n",
       "                                      cleaned_abtract  \\\n",
       "0   introduce bag context device regulated rewriti...   \n",
       "1   addition science citation indicators journals ...   \n",
       "2   player strategy pattern recognition pspr apply...   \n",
       "3   cinematic virtual reality vr aims provide imme...   \n",
       "4   increasing demand popularity multimedia stream...   \n",
       "5   motivation proteins consist multiple domains i...   \n",
       "6   give new topological definition automata exten...   \n",
       "7   process mapping computeintensive algorithms on...   \n",
       "8   android applications often rely alarms schedul...   \n",
       "9   consider deterministic anonymous distributed s...   \n",
       "10  distributed denial service attacks recently em...   \n",
       "11  increasing energy consumption commercial build...   \n",
       "12  integration many functions mobile devices ofte...   \n",
       "13  special purpose computers mainframes grew gene...   \n",
       "14  paper propose semiclosed microchip realize pro...   \n",
       "15  new way improve classification rate eegbased b...   \n",
       "16  paper describes study effects variation synapt...   \n",
       "17  development monitor based guidedtoleakymode wa...   \n",
       "18  paper propose uniform quantization likelihood ...   \n",
       "19  high power dissipation scanbased logic bist cr...   \n",
       "\n",
       "                                             keywords  \n",
       "0   [(tree, 8), (class, 4), (context, 3), (bc, 3),...  \n",
       "1   [(citation, 8), (journals, 4), (centrality, 4)...  \n",
       "2   [(game, 6), (ai, 4), (strategy, 3), (pspr, 3),...  \n",
       "3   [(stereo, 3), (cues, 3), (dasp, 3), (cinematic...  \n",
       "4   [(stream, 3), (software, 2), (problem, 2), (ch...  \n",
       "5   [(domains, 5), (protein, 4), (domain, 4), (oft...  \n",
       "6   [(automata, 3), (probabilistic, 2), (quantum, ...  \n",
       "7   [(channels, 4), (registers, 4), (pes, 3), (alg...  \n",
       "8   [(applications, 5), (alarms, 4), (android, 3),...  \n",
       "9   [(nodes, 3), (communications, 2), (functions, ...  \n",
       "10  [(attacks, 3), (attack, 3), (hierarchical, 3),...  \n",
       "11  [(energy, 11), (apportionment, 4), (wattshare,...  \n",
       "12  [(system, 7), (granularity, 5), (data, 4), (mo...  \n",
       "13  [(purpose, 3), (robot, 3), (special, 2), (comp...  \n",
       "14  [(cell, 9), (microchip, 8), (bath, 7), (cultur...  \n",
       "15  [(bci, 5), (results, 4), (methods, 3), (classi...  \n",
       "16  [(model, 6), (synaptic, 4), (certain, 4), (eeg...  \n",
       "17  [(input, 2), (materials, 2), (wave, 2), (devel...  \n",
       "18  [(implementation, 5), (uqle, 4), (algorithm, 4...  \n",
       "19  [(test, 5), (power, 4), (control, 4), (pattern...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf9fb5",
   "metadata": {},
   "source": [
    "# App.py (Don't run it here, run in vs or pycharm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "import\n",
    "# run this in terminal; python -m nltk.downloader all\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    # lower casing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove everything except letters and digits, and make lowercase for a continuous string\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    # tokenization\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    # remove stopwords\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "\n",
    "def get_keywords(text):\n",
    "    # Create a TextBlob object\n",
    "    blob = TextBlob(text)\n",
    "\n",
    "    # Extract a broader range of keywords based on POS tagging\n",
    "    keywords = [word for word, tag in blob.tags if tag in ('NN', 'NNS', 'NNP', 'NNPS',  # Nouns\n",
    "                                                           'JJ', 'JJR', 'JJS',  # Adjectives\n",
    "                                                           'RB', 'RBR', 'RBS')]  # Adverbs\n",
    "\n",
    "    # Count the most common keywords\n",
    "    keyword_counts = Counter(keywords)\n",
    "    most_common_keywords = keyword_counts.most_common(5)  # Get the top 5 most common keywords\n",
    "\n",
    "    return most_common_keywords\n",
    "\n",
    "\n",
    "# UI code=================\n",
    "st.title(\"Auto Keyword Extraction from Articles Text Using TextBlob\")\n",
    "uploaded_file = st.sidebar.file_uploader(\"Upload a file\", type=\"csv\")\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # read dataset\n",
    "    df = pd.read_csv(uploaded_file)\n",
    "    df = df[['abstract', 'authors']]\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.sample(n=20)\n",
    "    # apply on data\n",
    "    st.write(\"Uploaded File Preview\")\n",
    "    st.dataframe(df.head(3))\n",
    "\n",
    "    # Check if the 'abstract' column exists and is not empty\n",
    "    if 'abstract' in df.columns and not df['abstract'].empty:\n",
    "        df['clean_abstract'] = df['abstract'].apply(clean)\n",
    "        df['keywords'] = df['clean_abstract'].apply(get_keywords)\n",
    "\n",
    "\n",
    "\n",
    "        # Ensure keywords column contains only strings\n",
    "        df['keywords'] = df['keywords'].apply(lambda x: str(x) if not isinstance(x, str) else x)\n",
    "        # Handle NaN or None values by replacing them with an empty string\n",
    "        df['keywords'] = df['keywords'].fillna('')\n",
    "        st.write(\"Extracted Keywords Preview\")\n",
    "        st.dataframe(df)\n",
    "    else:\n",
    "        st.write(\"Uploaded Data must have abstract column...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b22c1",
   "metadata": {},
   "source": [
    "# Project 5: Text Summarization by Sentence Polarity Using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d352793d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Summary: I love this place. It's beautiful! The weather is great.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Example text\n",
    "text = \"I love this place. It's beautiful! The weather is great. I hate the traffic.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Summarize by filtering positive sentences\n",
    "summary = ' '.join([sentence.string for sentence in blob.sentences if sentence.sentiment.polarity > 0])\n",
    "\n",
    "print(\"Positive Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90a425",
   "metadata": {},
   "source": [
    "# Project 6: Formality Checker For Text Using Python | Formality Classifier | Analyzing Text for Formal and Informal Tone Using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bb1ccb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type here... or type \"exit\" to quit: \"Hey, what’s up? Wanna grab a coffee later? Hit me up if you're free.\"\n",
      "\n",
      "\n",
      "\n",
      "Formality: Informal\n",
      "Type here... or type \"exit\" to quit:  \"Dear Sir/Madam, I hope this message finds you well. I am writing to inquire about the status of my application. I would appreciate it if you could provide me with an update at your earliest convenience. Thank you for your time and consideration.\"\n",
      "\n",
      "\n",
      "\n",
      "Formality: Formal\n",
      "Type here... or type \"exit\" to quit: : \"Hey dude, can you send me the link to the movie? I’ve been meaning to watch it.\"\n",
      "\n",
      "\n",
      "\n",
      "Formality: Informal\n",
      "Type here... or type \"exit\" to quit: Please accept my apologies for the delay in responding. I look forward to hearing from you soon. If you have any questions, please feel free to contact me. I would be happy to assist you.\"\n",
      "\n",
      "\n",
      "\n",
      "Formality: Formal\n",
      "Type here... or type \"exit\" to quit: exit\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "# Expanded formal and informal word lists\n",
    "formal_words = set([\n",
    "    \"please\", \"thank you\", \"sincerely\", \"best regards\", \"kindly\", \"excuse me\", \"dear\", \"respectfully\", \n",
    "    \"honored\", \"appreciate\", \"cordially\", \"regards\", \"with respect\", \"for your consideration\", \"would you mind\", \n",
    "    \"grateful\", \"acknowledge\", \"thankful\", \"truly\", \"understanding\", \"yours truly\", \"faithfully\", \"congratulations\", \n",
    "    \"compliments\", \"with appreciation\", \"much obliged\", \"could you\", \"may I\", \"please be advised\", \"permit me\", \n",
    "    \"in compliance\", \"at your earliest convenience\", \"I look forward to\", \"please let me know\", \"to whom it may concern\", \n",
    "    \"please find attached\", \"appreciatively\", \"with all due respect\", \"it would be appreciated\", \"it is my honor\", \n",
    "    \"I would like to\", \"I would appreciate\", \"best wishes\", \"for your information\", \"kind regards\", \"thanks in advance\", \n",
    "    \"yours sincerely\", \"in regards to\", \"with gratitude\", \"in appreciation\", \"with best wishes\", \"could you kindly\", \n",
    "    \"looking forward to\", \"with the utmost respect\", \"please accept\", \"with warm regards\", \"with kind regards\", \n",
    "    \"at your service\", \"please confirm\", \"my pleasure\", \"respectfully yours\", \"I humbly\", \"we are grateful\", \n",
    "    \"please inform\", \"as per your request\", \"in your service\", \"as previously mentioned\", \"I trust\", \n",
    "    \"with respect and appreciation\", \"please advise\", \"your prompt response is appreciated\", \"I would be happy to\", \n",
    "    \"thank you very much\", \"I hope this finds you well\", \"I am writing to\", \"thank you for your time\", \n",
    "    \"it is my privilege\", \"your attention to this matter\", \"looking forward\", \"it would be my pleasure\", \"have a great day\", \n",
    "    \"gratefully\", \"best regards\", \"thank you for your cooperation\", \"with sincere regards\", \"it would be an honor\", \n",
    "    \"please be reminded\", \"in good faith\", \"with consideration\", \"your feedback is appreciated\", \"I would be delighted\", \n",
    "    \"please note\", \"with thanks\", \"with appreciation\", \"to be continued\", \"please take note\", \"I hope to hear from you\", \n",
    "    \"in conclusion\", \"yours faithfully\"\n",
    "])\n",
    "\n",
    "informal_words = set([\n",
    "    \"hey\", \"yo\", \"sup\", \"wanna\", \"gimme\", \"okay\", \"lol\", \"yo\", \"bro\", \"dude\", \"chill\", \"buddy\", \"what's up\", \"nah\", \n",
    "    \"bff\", \"lmao\", \"omg\", \"lolz\", \"rofl\", \"idk\", \"ttyl\", \"brb\", \"catch you later\", \"peace\", \"fam\", \"no worries\", \n",
    "    \"whatever\", \"cya\", \"k\", \"pls\", \"gotcha\", \"no prob\", \"tbh\", \"lmfao\", \"holla\", \"hype\", \"cray\", \"l8r\", \"yeah\", \n",
    "    \"yass\", \"babe\", \"yo dude\", \"dawg\", \"wut\", \"wth\", \"chillax\", \"bffl\", \"glhf\", \"fml\", \"smh\", \"wtf\", \"yo man\", \n",
    "    \"soo\", \"lolol\", \"lmao\", \"broke\", \"yolo\", \"lit\", \"savage\", \"hit me up\", \"stay woke\", \"good vibes\", \"let's go\", \n",
    "    \"hey yo\", \"wanna go\", \"brother\", \"homie\", \"sure thing\", \"let's catch up\", \"let me know\", \"ain't nobody\", \"hit me\", \n",
    "    \"chillin\", \"what's good\", \"grub\", \"swag\", \"bored\", \"no cap\", \"vibe\", \"gurl\", \"hey there\", \"come thru\", \"y'all\", \n",
    "    \"fomo\", \"yo girl\", \"that's fire\", \"send it\", \"catch ya\", \"mad\", \"lucky\", \"peace out\", \"squad\", \"g'day\", \"yo fam\", \n",
    "    \"slay\", \"dank\", \"flex\", \"slay queen\", \"cringe\", \"bruh\", \"rly\", \"imma\", \"who's down\", \"bored af\", \"lowkey\", \n",
    "    \"highkey\", \"for real\", \"sheesh\", \"bet\", \"on fleek\", \"g'day mate\", \"dank memes\", \"smash\", \"hit me up later\", \"nah fam\", \n",
    "    \"goals\", \"peace bro\", \"weird flex\", \"lit af\", \"sick\", \"that's dope\", \"howdy\", \"gimme a sec\"\n",
    "])\n",
    "\n",
    "# Custom function to check for formal tone\n",
    "def check_formality(text):\n",
    "    # Analyze the text with TextBlob for sentiment analysis (polarity)\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # Lowercase the words and split them\n",
    "    words = set(blob.words.lower())\n",
    "    \n",
    "    # Count formality score based on vocabulary\n",
    "    formal_score = sum(1 for word in words if word in formal_words)\n",
    "    informal_score = sum(1 for word in words if word in informal_words)\n",
    "    \n",
    "    # If formal words dominate, it's Formal, if informal words dominate, it's Informal\n",
    "    if formal_score > informal_score:\n",
    "        return \"Formal\"\n",
    "    elif informal_score > formal_score:\n",
    "        return \"Informal\"\n",
    "    else:\n",
    "        # Check sentiment polarity to classify borderline cases\n",
    "        polarity = blob.sentiment.polarity\n",
    "        if polarity > 0.1:\n",
    "            return \"Informal\"  # More likely to be informal if sentiment is positive\n",
    "        elif polarity < -0.1:\n",
    "            return \"Formal\"  # More likely to be formal if sentiment is negative\n",
    "        else:\n",
    "            return \"Neutral\"  # If it's truly neutral\n",
    "\n",
    "# Test with user input\n",
    "while True:\n",
    "    text_input = input('Type here... or type \"exit\" to quit: ')\n",
    "    \n",
    "    if text_input.lower() == \"exit\":\n",
    "        break\n",
    "    # Check formality of the text\n",
    "    formality = check_formality(text_input)\n",
    "    print(\"\\n\\n\")\n",
    "    print(f\"Formality: {formality}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e6be4c",
   "metadata": {},
   "source": [
    "# Exmaples\n",
    "\n",
    "informal example:\n",
    "\n",
    "Text: \"Hey, what’s up? Wanna grab a coffee later? Hit me up if you're free.\"\n",
    "\n",
    "Text: \"Yo, you coming to the party tonight? It's gonna be lit! Let me know!\"\n",
    "\n",
    "Text: \"Lol, that was hilarious! You should’ve seen the look on his face! Anyway, catch you later!\"\n",
    "\n",
    "Text: \"Sup, bro? Haven't heard from you in a while. Wanna hang out this weekend?\"\n",
    "\n",
    "Text: \"Hey dude, can you send me the link to the movie? I’ve been meaning to watch it.\"\n",
    "\n",
    "\n",
    "formal examples;\n",
    "\n",
    "Text: \"Dear Sir/Madam, I hope this message finds you well. I am writing to inquire about the status of my application. I would appreciate it if you could provide me with an update at your earliest convenience. Thank you for your time and consideration.\"\n",
    "\n",
    "Text: \"I would like to take this opportunity to express my sincere gratitude for your continued support. Please find attached the requested documents for your review. If you require any further information, kindly do not hesitate to reach out.\"\n",
    "\n",
    "Text: \"Please accept my apologies for the delay in responding. I look forward to hearing from you soon. If you have any questions, please feel free to contact me. I would be happy to assist you.\"\n",
    "\n",
    "Text: \"I trust that this email finds you well. I would like to request a meeting at your earliest convenience to discuss the next steps for our project. Thank you for your attention to this matter.\"\n",
    "\n",
    "Text: \"With respect, I would like to inform you that your application has been successfully received. We will review it thoroughly and provide feedback as soon as possible. Thank you for your patience.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16dc7e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type here... or type exit  :\"Hey, what’s up? Wanna grab a coffee later? Hit me up if you're free.\"\n",
      "Text: '\"Hey, what’s up? Wanna grab a coffee later? Hit me up if you're free.\"' | Formality: Informal\n",
      "Type here... or type exit  :\"Hey dude, can you send me the link to the movie? I’ve been meaning to watch it.\"\n",
      "Text: '\"Hey dude, can you send me the link to the movie? I’ve been meaning to watch it.\"' | Formality: Informal\n",
      "Type here... or type exit  :\"Dear Sir/Madam, I hope this message finds you well. I am writing to inquire about the status of my application. I would appreciate it if you could provide me with an update at your earliest convenience. Thank you for your time and consideration.\"\n",
      "Text: '\"Dear Sir/Madam, I hope this message finds you well. I am writing to inquire about the status of my application. I would appreciate it if you could provide me with an update at your earliest convenience. Thank you for your time and consideration.\"' | Formality: Informal\n",
      "Type here... or type exit  :exit\n",
      "Text: 'exit' | Formality: Informal\n",
      "Type here... or type exit  :eixt\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    text = input(\"Type here... or type exit  :\")\n",
    "    if text == \"eixt\":\n",
    "        break\n",
    "    else:\n",
    "        # Check formality of the text\n",
    "        formality = check_formality(text)\n",
    "        # Print the result\n",
    "        print(f\"Text: '{text}' | Formality: {formality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e1304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
